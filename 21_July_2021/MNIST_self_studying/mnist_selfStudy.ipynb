{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12061b08",
   "metadata": {},
   "source": [
    "``Tensorflow`` is a library for both machine and deep learning. ``Keras`` is an API designed for deep learning and for human, now it has been included in Tensorflow library.\n",
    "\n",
    "``MNIST`` is a dataset that was built-in keras, it contains 60,000 training data and 10,000 test sets 28*28 example of handwriting photo.\n",
    "\n",
    "Supplementing, ``CIFA10`` also is a dataset that built-in keras, it commonly uses for image classifying, it consists 60,000 32*32 colour images in 10 classes, 6,000 per class, 50,000 training images and 10,000 test image.\n",
    "\n",
    "``Matplotlib`` is a Python graphical, data visualization API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist # , cifa10\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40dd24",
   "metadata": {},
   "source": [
    "Loading both training and test data.\n",
    "\n",
    "MNIST\n",
    "\n",
    "``(x_train, y_train), (x_test, y_test) = mnist.load_data()``\n",
    "\n",
    "CIFA10\n",
    "\n",
    "``(x_train, y_train), (x_test, y_test) = cifa10.load_data()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21e4f0",
   "metadata": {},
   "source": [
    "Importing keras's models and layers to construct neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ab154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a248e8",
   "metadata": {},
   "source": [
    "Using ``.shape`` to check loaded dataset volume and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce01aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003823e",
   "metadata": {},
   "source": [
    "Building neural network using ``models.Sequential()``.\n",
    "\n",
    "``Sequential()`` model\n",
    "\n",
    "A ``Sequential`` model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "``layers API``\n",
    "\n",
    "Layers are the basic building blocks of neural networks. A layer consists of a tensor-in tensor-out computation function(the layer's ``call`` method) and some state, held in Tensorflow variables(the ``layer``'s weights).\n",
    "A ``Layers`` instance is callable, much like a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b6c22",
   "metadata": {},
   "source": [
    "``Dense layer``, ``.Dense()``\n",
    "\n",
    "Just your regular densely-connected NN layer.\n",
    "\n",
    "```\n",
    "layers.Dense(\n",
    "    units,\n",
    "    activation=\"...\",\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"...\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs # Popular kwarg \"input_shape\"\n",
    ")\n",
    "```\n",
    "\n",
    "``units``\n",
    "\n",
    "Positive integer, dimensionality of the output space.\n",
    "\n",
    "``activation``\n",
    "\n",
    "Activation function to use.\n",
    "\n",
    "``use_bias``\n",
    "\n",
    "Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "``kernel_initializer``\n",
    "\n",
    "Initializer fo the ``kernel`` weights matrix.\n",
    "\n",
    "``bias_initializer``\n",
    "\n",
    "Initializer for the bias vector.\n",
    "\n",
    "``kernel_regularizer``\n",
    "\n",
    "Regularizer function applied to the ``kernel`` weights matrix.\n",
    "\n",
    "``bias_regularizer``\n",
    "\n",
    "Regularizer function applied to bias vector.\n",
    "\n",
    "``activity_regularizer``\n",
    "\n",
    "Regularizer function applied to the output of the layer(its \"activation\").\n",
    "\n",
    "``kernel_constraint``\n",
    "\n",
    "Constraint function applied to the ``kernel`` weights matrix.\n",
    "\n",
    "``bias_constraint``\n",
    "\n",
    "Constraint function applied to the bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a48316",
   "metadata": {},
   "source": [
    "``Input shape``, ``input_shape``\n",
    "\n",
    "The most common situation would be a 2D input with shape\n",
    "``(batch_size, input_dim)``.\n",
    "\n",
    "``Output shape``\n",
    "\n",
    "For a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, units)``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62edaeff",
   "metadata": {},
   "source": [
    "``activation``\n",
    "\n",
    "Availables,\n",
    "\n",
    "``relu``\n",
    "\n",
    "Applies the rectified linear unit activation function.\n",
    "\n",
    "``sigmoid``\n",
    "\n",
    "Sigmoid activation function ``sigmoid(x) = 1 / (1 + exp(-x))``.\n",
    "\n",
    "``softmax``\n",
    "\n",
    "Softmax converts a vector of values to a probability distribution.\n",
    "The elements of the output vector are in range(0, 1) and sum to 1.\n",
    "\n",
    "Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.\n",
    "\n",
    "The softmax of each vector x is computed as ``exp(x) / tf.reduce_sum(exp(x))``\n",
    "\n",
    "\n",
    "``softsign``\n",
    "\n",
    "Softsign activation function, softsign(x) = x / (abs(x) + 1).\n",
    "\n",
    "``tanh``\n",
    "\n",
    "Hyperbolic tangent activation function.\n",
    "\n",
    "``selu``\n",
    "\n",
    "The Scaled Exponential Linear Unit(SELU) activation function is defined as:\n",
    "\n",
    "``\n",
    "if x > 0: return scale * x\n",
    "\n",
    "if x < 0: return scale * alpha * (exp(x) - 1)\n",
    "\n",
    "``\n",
    "\n",
    "where ``alpha`` and ``scale`` are pre-defined constants (``alpha=1.67326324`` and ``scale=1.05070098``).\n",
    "\n",
    "Basically, the SELU activation function multiplies ``scale`` (> 1) with the output of the ``tf.keras.activations.elu`` function to ensure a slope larger than one for positive inputs.\n",
    "\n",
    "``elu``\n",
    "\n",
    "Exponential Linear Unit.\n",
    "\n",
    "The exponential linear unit (ELU) with ``alpha > 0`` is: ``x`` if ``x > 0`` and ``alpha * (exp(x) - 1)`` if ``x < 0`` The ELU hyperparameter ``alpha`` controls the value to which an ELU saturates for negative net inputs. ELUs diminish the vanishing gradient effect.\n",
    "\n",
    "``exponential``\n",
    "\n",
    "Exponential activation function.\n",
    "\n",
    "Returns\n",
    "\n",
    "Tensor with exponential activation: ``exp(x)``.\n",
    "\n",
    "Noted that there are ``advance activation`` layers, references when needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25195c69",
   "metadata": {},
   "source": [
    "``Add layer``, ``.Add()``, ``.add()``\n",
    "\n",
    "Layer that adds a list of inputs.\n",
    "It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN, Neural Network\n",
    "\n",
    "NN = models.Sequential()\n",
    "\n",
    "NN.add(layers.Dense(128, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "NN.add(layers.Dense(64, activation=\"relu\")) # relu, LeakyRelu\n",
    "NN.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62cb09",
   "metadata": {},
   "source": [
    "``Conv2D layer``, ``Conv2D()``\n",
    "\n",
    "2D convolution layer(e.g. spatial convolution over images).\n",
    "\n",
    "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "\n",
    "```\n",
    "Conv2D(\n",
    "    filters, kernel_size,\n",
    "   \n",
    "    activation=\"...\"\n",
    "\n",
    "    kernel_initializer=\"...\"\n",
    "    \n",
    "    **kwargs # input_shape=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "Noted that there are more function can be used, references when needed.\n",
    "\n",
    "``filters``\n",
    "\n",
    "Integer, the dimensionality of the output space(i.e. the number of output filters in the convolution).\n",
    "\n",
    "``kernel_size``\n",
    "\n",
    "An integer or tuple/list of 2 intergers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "\n",
    "``Input shape``, ``input_shape``\n",
    "\n",
    "4+D tensor with shape: ``batch_shape + (channels, rows, cols)`` if ``data_format='channels_first'`` or 4+D tensor with shape: ``batch_shape + (rows, cols, channels)`` if ``data_format='channels_last'``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185cce37",
   "metadata": {},
   "source": [
    "``MaxPooling2D layer``, ``MaxPooling2D()``\n",
    "\n",
    "Max pooling operation for 2D spartial data.\n",
    "\n",
    "Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by ``pool_size``) for each channel of the input. The window is shifted by ``strides`` along each dimension.\n",
    "\n",
    "``MaxPooling2D((pool_size))``\n",
    "\n",
    "\n",
    "``Flatten layer``, ``Flatten()``\n",
    "\n",
    "Flattens the input. Does not affect the batch size.\n",
    "\n",
    "Note: if inputs are shaped ``(batch,)`` without a feature axis, then flattening adds an extra channel dimension and output shape is ``(batch, 1)``.\n",
    "\n",
    "\n",
    "``HeUniform class``, ``he_uniform``\n",
    "\n",
    "He uniform variance scaling initializer.\n",
    "\n",
    "Also available via the shortcut function ``tf.keras.initializers.he_uniform``.\n",
    "\n",
    "Draws samples from a uniform distribution within ``[-limit, limit]``, where ``limit = sqrt(6 / fan_in)`` (``fan_in`` is the number of input units in the weight tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN, Convolution Neural Network\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "NN = models.Sequential()\n",
    "\n",
    "NN.add(\n",
    "    Conv2D(\n",
    "        32, (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "        input_shape=(28, 28, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "NN.add(MaxPooling2D((2, 2)))\n",
    "NN.add(Flatten())\n",
    "NN.add(Dense(100, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "NN.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbec1b1",
   "metadata": {},
   "source": [
    "```Optimizers```\n",
    "\n",
    "Usage with ``compile()`` & ``fit()``\n",
    "\n",
    "An optimizer is one of the two arguments required for compiling a Keras model.\n",
    "\n",
    "You can either instantiate an optimizer before passing it to ``model.compile()``, or you can pass it by its string identifier.\n",
    "\n",
    "Usage in a custom training loop\n",
    "\n",
    "When writing a custom training loop, you would retrieve gradients via a ``tf.GradientTape instance``, then call ``optimizer.apply_gradients()`` to update your weights:\n",
    "\n",
    "``Official documentation example``\n",
    "```\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x, y in dataset:\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        logits = model(x)\n",
    "        # Loss value for this batch.\n",
    "        loss_value = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of loss wrt the weights.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # Update the weights of the model.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    \n",
    "```\n",
    "Note that when you use ``apply_gradients``, the optimizer does not apply gradient clipping to the gradients: if you want gradient clipping, you would have to do it by hand before calling the method.\n",
    "\n",
    "Availabel optimizers\n",
    "\n",
    "```\n",
    "SGD\n",
    "RMSprop\n",
    "Adam\n",
    "Adadelta\n",
    "Adagrad\n",
    "Adamax\n",
    "Nadam\n",
    "Ftrl\n",
    "\n",
    "```\n",
    "\n",
    "Noted: There are more about optimizers, references when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b9491",
   "metadata": {},
   "source": [
    "``Losses``, ``loss``\n",
    "\n",
    "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
    "\n",
    "``Available losses``\n",
    "\n",
    "Note that all losses are available both via a class handle and via a function handle. The class handles enable you to pass configuration arguments to the constructor (e.g. ``loss_fn = CategoricalCrossentropy(from_logits=True)``), and they perform reduction by default when used in a standalone way.\n",
    "\n",
    "There are plenty loss functions, references when needed.\n",
    "\n",
    "``CategoricalCrossentropy``, ``categorical_crossentropy``\n",
    "\n",
    "Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a ``one_hot`` representation.\n",
    "\n",
    "``Metrics``, ``metrics``\n",
    "\n",
    "A metric is a function that is used to judge the performance of your model.\n",
    "\n",
    "Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric.\n",
    "\n",
    "There are plenty metrics, references when needed.\n",
    "\n",
    "``Accuracy class``, ``accuracy``\n",
    "\n",
    "Calculates how often predictions equal labels.\n",
    "\n",
    "This metric creates two local variables, ``total`` and ``count`` that are used to compute the frequency with which ``y_pred`` matches ``y_true``. This frequency is ultimately returned as ``binary accuarcy``: and idempotent operation that simply divides ``total`` by ``count``.\n",
    "\n",
    "if ``sample_weight`` is ``None``, weights default to 1. Use ``sample_weight`` of 0 to mask values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c015d",
   "metadata": {},
   "source": [
    "``summary()``\n",
    "\n",
    "Once a model is \"built\", you can call its ``summary()`` method to display its contents.\n",
    "\n",
    "Here, we display the built model, NN's contents, thru ``summary()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250ebc5",
   "metadata": {},
   "source": [
    "``Reshape layer``, ``.reshape()``\n",
    "\n",
    "```\n",
    "Reshape class\n",
    "\n",
    "tf.keras.layers.Reshape(target_shape, **kwargs)\n",
    "\n",
    "```\n",
    "\n",
    "Layer that reshapes inputs into the given shape.\n",
    "\n",
    "``Input shape``, ``input_shape``\n",
    "\n",
    "Arbitrary, although all dimensions in the input shape must be known/fixed. Use the keyword argument ``input_shape`` (tuple of integers, does not include the samples/batch size axis) when using this layer as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e230b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NL = X_train.reshape((-1, 28, 28, 1))\n",
    "X_test_NL = X_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2093baa",
   "metadata": {},
   "source": [
    "Ensuring the input features are scaled between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NL = X_train_NL.astype(\"float32\") / 255\n",
    "X_test_NL = X_test_NL.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f13468",
   "metadata": {},
   "source": [
    "``One-hot``\n",
    "\n",
    "In digital circuits and machine learning, a ``one-hot`` is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "In machine learning, ``one-hot`` encoding is a frequently used method to deal with categorical data. Because many machine learning models need their input variables to be numeric, categorical variables need to be transformed in the pre-processing part.\n",
    "\n",
    "1 meaning TRUE, 0 meaning FALSE\n",
    "\n",
    "``to_categorical function``, ``to_categorical()``\n",
    "\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "E.g. for use with ``categorical_crossentropy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train_OneHot = to_categorical(Y_train)\n",
    "Y_test_OneHot = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a557f",
   "metadata": {},
   "source": [
    "Checks the reshaped training dataset's volume and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b09117",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7071377",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = NN.fit(\n",
    "            X_train_NL, Y_train_OneHot,\n",
    "            validation_data = (X_test_NL, Y_test_OneHot),\n",
    "            epochs = 13, batch_size = 128,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = NN.predict_classes(X_test_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13602a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prob = NN.predict(X_test_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57617c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021368e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37cf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
